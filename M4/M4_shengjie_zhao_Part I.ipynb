{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natural-Language-Processing-YU/Module-5-Assignment/blob/main/scripts/Part%20I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KBtJv0BsJxNx"
      },
      "source": [
        "##Part I:  Vector Semantics and Motivation for Word Embeddings\n",
        "\n",
        "It is important to understand the words meaning (recall semantics) AND their context. Words that are seen in the similar context also often have similar meaning. The distributional hypothesis expresses this phenomenon saying that there is a link in similarity in how words are distributed and their likeness.  Vector semantics is the concept of learning representations of meanings of words â€“ called embeddingsâ€”from their distributions in a corpus or corpora. Fundamentally, we are asking the question with NLP: how might we represent the meaning of a word and interpret it?\n",
        "\n",
        "A word embedding is simply a to represent words in a numerical context -- a vector.  This is important because Neural Networks and Machine Learning models don't learn on the text itself, but the numerical representation of the text. In fact, there is typically an \"embedding layer\" as part of the simplest NLP-based neural networks as you will find.\n",
        "\n",
        "The simplest way to show this is called a one-hot vector, other forms include term frequencies of words (as we have seen with Bayesian models), Term Frequency-Inverse Document Frequency, which normalizes terms across documents, and distributional representations, which are context-based encodings that help derive similarity-- i.e., \"queen is to female as king is to male\".\n",
        "\n",
        "We will start simple and discuss some of the challenges, then move to more complex transformations.\n",
        "\n",
        "## Setup\n",
        "As part of completing the assignment, you will see that there are areas in the note book for you to complete your own coding input.\n",
        "\n",
        "It will be look like following:\n",
        "```\n",
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "'Some coding activity for you to complete'\n",
        "### END CODE HERE ###\n",
        "\n",
        "```\n",
        "Please be sure to fill these code snippets out as you turn in your assignment.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mKPWU-Ip2AZL"
      },
      "source": [
        "##1.1 One-hot vector\n",
        "\n",
        "A one-hot vector helps to translate categorical or sequential data to something that is machine readable and also does not have an impact on your model. Each word in the sequence is given a binary encoding and is mapped to a vector of the length of the the input. This is a common pre-processing step for the input layer in a neural network.\n",
        "\n",
        "One hot encoding assigns a unique code for each unique word. As an example, we can take the following sentence and convert it to a one-hot vector.\n",
        "\n",
        "\"Live as if you were to die tomorrow. Learn as if you were to live forever\"\n",
        "\n",
        "We will use NLTK to tokenize the sentence, then Sci-Kit Learn to apply the one-hot encoder. Note, that SK-Learn will apply a single value for a unique word in a vector which is great for categorical representations. This one-hot encoding has traditionally been used for feeding categorical data to many scikit-learn estimators in shallow learning models such as notably linear models and SVMs with the standard kernels.\n",
        "\n",
        "Note: This approach is inefficient. A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine you have 10,000 words in the vocabulary. To one-hot encode each word, you would create a vector where 99.99% of the elements are zero.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "V5pWPsfD2BfJ",
        "outputId": "f71adb89-38e2-44bc-edf0-7ee4ceb16921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " One-Hot Encoded Vector using SKLearn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\shengjie\n",
            "[nltk_data]     zhao\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "c:\\Users\\shengjie zhao\\anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>as</th>\n",
              "      <th>die</th>\n",
              "      <th>forever</th>\n",
              "      <th>if</th>\n",
              "      <th>learn</th>\n",
              "      <th>live</th>\n",
              "      <th>to</th>\n",
              "      <th>tomorrow</th>\n",
              "      <th>were</th>\n",
              "      <th>you</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     as  die forever   if learn live   to tomorrow were  you\n",
              "0   0.0  0.0     0.0  0.0   0.0  1.0  0.0      0.0  0.0  0.0\n",
              "1   1.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  0.0\n",
              "2   0.0  0.0     0.0  1.0   0.0  0.0  0.0      0.0  0.0  0.0\n",
              "3   0.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  1.0\n",
              "4   0.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  1.0  0.0\n",
              "5   0.0  0.0     0.0  0.0   0.0  0.0  1.0      0.0  0.0  0.0\n",
              "6   0.0  1.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  0.0\n",
              "7   0.0  0.0     0.0  0.0   0.0  0.0  0.0      1.0  0.0  0.0\n",
              "8   0.0  0.0     0.0  0.0   1.0  0.0  0.0      0.0  0.0  0.0\n",
              "9   1.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  0.0\n",
              "10  0.0  0.0     0.0  1.0   0.0  0.0  0.0      0.0  0.0  0.0\n",
              "11  0.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  1.0\n",
              "12  0.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  1.0  0.0\n",
              "13  0.0  0.0     0.0  0.0   0.0  0.0  1.0      0.0  0.0  0.0\n",
              "14  0.0  0.0     0.0  0.0   0.0  1.0  0.0      0.0  0.0  0.0\n",
              "15  0.0  0.0     1.0  0.0   0.0  0.0  0.0      0.0  0.0  0.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from numpy import argmax\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "nltk.download('punkt')\n",
        "\n",
        "#%matplotlib inline\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# define input string\n",
        "data = 'Live as if you were to die tomorrow. Learn as if you were to live forever'\n",
        "#tokenize that string\n",
        "wordlist = nltk.word_tokenize(data.lower())\n",
        "#create a vector representation of the wordlist\n",
        "wordlist_clean = []\n",
        "\n",
        "for i in wordlist: # Go through every word in your tokens list\n",
        "    if (i not in string.punctuation):  # remove punctuation\n",
        "        wordlist_clean.append(i)\n",
        "# define universe of possible input values\n",
        "wordlist_clean_df = pd.DataFrame(data=wordlist_clean, columns=['words'])\n",
        "\n",
        "#encode using scki-kit learn\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "one_hot_encoder.fit(wordlist_clean_df)\n",
        "wordlist_clean_df_encoded = one_hot_encoder.transform(wordlist_clean_df)\n",
        "wordlist_clean_df_encoded = pd.DataFrame(data=wordlist_clean_df_encoded, columns=one_hot_encoder.categories_)\n",
        "print('\\n\\n One-Hot Encoded Vector using SKLearn')\n",
        "display(wordlist_clean_df_encoded)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aTRMr2m-oxRO"
      },
      "source": [
        "##1.2 Encoding as a dense  - Singular Value Decomposition\n",
        "A second approach you might try is to encode each word using a unique number. This helps with reducing dimensionality and attempts to address the problem of very large sparse matrices. Continuing the example above, you could assign 1 to \"live\", 2 to \"the\", and so on. You could then encode the sentence \"The cat sat on the mat\" as a dense vector. Now, instead of a sparse vector, you now have a dense one. A dense vector is a vector where all elements are populated with a non-zero value.\n",
        "\n",
        "There are several challenges:\n",
        "\n",
        "1.   The integer-encoding is arbitrary (it does not capture any relationship between words)\n",
        "2.   An integer-encoding can be challenging for a model to interpret. A linear classifier, for example, learns a single weight for each feature. Because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful.\n",
        "3.  Word order is ignored.\n",
        "4.  Raw absolute frequency counts of words do not necessarily represent the meaning of the text properly\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "0C-2Kg9Dq05v",
        "outputId": "57b17d78-9f62-4edf-cde4-386fb0d680eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\shengjie\n",
            "[nltk_data]     zhao\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>as</th>\n",
              "      <th>die</th>\n",
              "      <th>forever</th>\n",
              "      <th>if</th>\n",
              "      <th>learn</th>\n",
              "      <th>live</th>\n",
              "      <th>to</th>\n",
              "      <th>tomorrow</th>\n",
              "      <th>were</th>\n",
              "      <th>you</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>as</td>\n",
              "      <td>die</td>\n",
              "      <td>forever</td>\n",
              "      <td>if</td>\n",
              "      <td>learn</td>\n",
              "      <td>live</td>\n",
              "      <td>to</td>\n",
              "      <td>tomorrow</td>\n",
              "      <td>were</td>\n",
              "      <td>you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   as  die  forever  if  learn  live  to  tomorrow  were  you\n",
              "0  as  die  forever  if  learn  live  to  tomorrow  were  you\n",
              "1   2    1        1   2      1     2   2         1     2    2"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from numpy import argmax\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "nltk.download('punkt')\n",
        "\"\"\"## Default Style Settings\n",
        "matplotlib.rcParams['figure.dpi'] = 150\n",
        "pd.options.display.max_colwidth = 200\n",
        "#%matplotlib inline\"\"\"\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# define input string\n",
        "data = 'Live as if you were to die tomorrow. Learn as if you were to live forever'\n",
        "#tokenize that string\n",
        "wordlist = nltk.word_tokenize(data.lower())\n",
        "#create a vector representation of the wordlist\n",
        "wordlist_clean = []\n",
        "\n",
        "for i in wordlist: # Go through every word in your tokens list\n",
        "    if (i not in string.punctuation):  # remove punctuation\n",
        "        wordlist_clean.append(i)\n",
        "# define universe of possible input values\n",
        "wordlist_clean_df = pd.DataFrame(data=wordlist_clean, columns=['words'])\n",
        "dense_vector = np.unique(wordlist_clean_df, return_counts=True)\n",
        "dense_vector_df = pd.DataFrame(data=dense_vector, columns = np.unique(wordlist_clean_df))\n",
        "display(dense_vector_df)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X5217etS2QW5"
      },
      "source": [
        "##1.3 Text Vectorization\n",
        "\n",
        "\n",
        "*   Overview\n",
        "*   N-Gram Bag of Words\n",
        "*   Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "*   Document Similarity: Cosine Similarity, Jaccard Similarity, Euclidian Similarity\n",
        "*   Topic Modeling Exercise\n",
        "\n",
        "### Why do we do it?\n",
        "These subsquent categories of text vectorizations are ways to derive similarity from text documents. This is useful for NLP tasks such as topic modeling -- where we aim to show the relationship between documents via a category or topic. You will see how TF-IDF can be used to support topic modeling.\n",
        "\n",
        "Here are some text vectorization approaches in summary:\n",
        "![Text Vectorization Approaches](https://drive.google.com/uc?export=view&id=12GYWDaK5_offSn3Gy-hv_KpuTc4A_mGA)\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vKq4TXKo1zvi"
      },
      "source": [
        "\n",
        "### 1.3.1 N-Gram Bag-Of Words Model\n",
        "You've already learned the bag-of words model above with one-hot encoding and dense vectorization! We are counting the frequencies of words in the matrix in a dense representation of the word vector. What happens if we took some steps to improve the Bag-of-Words model by incorporating the n-gram approach we have learned earlier in the class.\n",
        "\n",
        "What does this do?\n",
        "If our goal is to identify words in texts that represent meaning of that text, then recall that taking the bi-gram, tri-gram, or n-gram of a corpus allows us to bring in context via the word order. With a simple BOW approach, no word order is considers. Moreover, we can filter words based on distributional counts -- that is, term frequencies. Imagine that the counts of a word fall into say a Gaussian (normal) Distribution across a number of corpora. We can use the distribution to filter out salient word-phrases or sequences in which we can infer the meaning of the text. Finally, we can apply weights to the frequency counts -- similar to weight vector in a NN-- in which those weights have an impact on word relationships or salience.\n",
        "\n",
        "\n",
        "![Bag of Words](https://drive.google.com/uc?export=view&id=1btCVz_8JWYTvE73qGLCRZb7nXg-kCiDU)\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-gR43ePKDKlu"
      },
      "source": [
        "#### 1.3.1.2 Example: N-Gram Bag-Of Words Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "cv1H2Ha-0RMP",
        "outputId": "9861c66f-07ad-4762-fc0f-bdc7fbaa4ded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['The sky is blue and beautiful.' 'Love this blue and beautiful sky!'\n",
            " 'The quick brown fox jumps over the lazy dog.'\n",
            " \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\"\n",
            " 'I love green eggs, ham, sausages and bacon!'\n",
            " 'The brown fox is quick and the blue dog is lazy!'\n",
            " 'The sky is very blue and the sky is very beautiful today'\n",
            " 'The dog is lazy but the brown fox is quick!']\n",
            "==================================================\n",
            "['sky blue beautiful' 'love blue beautiful sky'\n",
            " 'quick brown fox jumps lazy dog'\n",
            " 'kings breakfast sausages ham bacon eggs toast beans'\n",
            " 'love green eggs ham sausages bacon' 'brown fox quick blue dog lazy'\n",
            " 'sky blue sky beautiful today' 'dog lazy brown fox quick']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\shengjie\n",
            "[nltk_data]     zhao\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m bv_matrix \u001b[39m=\u001b[39m bv\u001b[39m.\u001b[39mfit_transform(norm_corpus)\n\u001b[0;32m     66\u001b[0m bv_matrix \u001b[39m=\u001b[39m bv_matrix\u001b[39m.\u001b[39mtoarray()\n\u001b[1;32m---> 67\u001b[0m vocab \u001b[39m=\u001b[39m bv\u001b[39m.\u001b[39;49mget_feature_names()\n\u001b[0;32m     68\u001b[0m pd\u001b[39m.\u001b[39mDataFrame(bv_matrix, columns\u001b[39m=\u001b[39mvocab)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
          ]
        }
      ],
      "source": [
        "#example from: https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/exercise/5-text-vectorization.html#\n",
        "\n",
        "import pandas as pd                        # Python library for pandas - data maniplation\n",
        "import numpy as np                         # Python library for numpy -- matrix algebra library\n",
        "import matplotlib                          # Python library for matplotlib -- visual display of data\n",
        "import matplotlib.pyplot as plt            # Python library for matplotlib -- visual display of data\n",
        "import nltk                                # Python library for NLP\n",
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "nltk.download('stopwords')                 # package for stop words\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "## Default Style Settings\n",
        "matplotlib.rcParams['figure.dpi'] = 150\n",
        "pd.options.display.max_colwidth = 200\n",
        "#%matplotlib inline\n",
        "\n",
        "corpus = [\n",
        "    'The sky is blue and beautiful.', 'Love this blue and beautiful sky!',\n",
        "    'The quick brown fox jumps over the lazy dog.',\n",
        "    \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
        "    'I love green eggs, ham, sausages and bacon!',\n",
        "    'The brown fox is quick and the blue dog is lazy!',\n",
        "    'The sky is very blue and the sky is very beautiful today',\n",
        "    'The dog is lazy but the brown fox is quick!'\n",
        "]\n",
        "labels = [\n",
        "    'weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather',\n",
        "    'animals'\n",
        "]\n",
        "\n",
        "corpus = np.array(corpus) # np.array better than list\n",
        "corpus_df = pd.DataFrame({'Document': corpus, 'Category': labels})\n",
        "corpus_df\n",
        "\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I | re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = wpt.tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)\n",
        "\n",
        "norm_corpus = normalize_corpus(corpus)\n",
        "print(corpus)\n",
        "print(\"=\"*50)\n",
        "print(norm_corpus)\n",
        "\n",
        "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n",
        "bv = CountVectorizer(ngram_range=(2, 2))\n",
        "bv_matrix = bv.fit_transform(norm_corpus)\n",
        "\n",
        "bv_matrix = bv_matrix.toarray()\n",
        "vocab = bv.get_feature_names()\n",
        "pd.DataFrame(bv_matrix, columns=vocab)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Be0ML9kADimc"
      },
      "source": [
        "### 1.3.2 Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "As an extension of the BOW model, we can weight the frequency (counts) of the terms in a document by considering its *dispersion*. Fundamentally, we are taken the total frequency of a word and dividing it by the number of documents with that term -- this gives us term frequency.\n",
        "\n",
        "\n",
        "Then we take the inverse The formula for TF-IDF will look something like:\n",
        "\n",
        "\n",
        "*   Term Frequency(TF): the number of times a word appears in a document. These are the raw absolute frequency counts of the words in the BOW model.\n",
        "*   Inverse Document Frequency(IDF): total documents in corpus over number of documents with term.\n",
        "\n",
        "> $\\textit{TF-IDF} = {tf \\times idf}$\n",
        "\n",
        "Here, the general idea is that we can extropolate the meaningful words from a corpus by inversing their frequency. For example, \"The\" in the corpus may be frequently observed, but does not garner meaning. We can use this for keyword extraction, and information retrieval tasks.\n",
        "\n",
        "Let's normalize this function to account for divide-by-zero erros and to also smooth the weighting scheme.\n",
        "\n",
        "Addressing divide-by-zero errors. Similar to Laplace Smoothing techniques, we will typically add one to the IDF formula:\n",
        "\n",
        "> $\\textit{IDF} = 1 + log\\frac{N}{1+df}$\n",
        "\n",
        "We also might normalize the final IF-IDF function using an L2 Norm (see more in Jurafsky, Chapter 6).\n",
        "\n",
        "> $\\textit{TF-IDF}_{normalized} = \\frac{tf \\times idf}{\\sqrt{(tf\\times idf)^2}}$\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X4tJ1Yrbbufr"
      },
      "source": [
        "#### 1.3.2.1 Example: TF-IDF Usage\n",
        "In our example, we use the TfidfTransformer function to apply L2 norms and smoothing techniques.\n",
        "\n",
        "```\n",
        "tt = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True)\n",
        "```\n",
        "\n",
        "\n",
        "Let's use the same corpus from above in our example for TD-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "6KROXdPWWRlv",
        "outputId": "08593b47-201f-469d-fa4e-39a5faa3c464"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m tt_matrix \u001b[39m=\u001b[39m tt\u001b[39m.\u001b[39mfit_transform(cv_matrix)\n\u001b[0;32m     23\u001b[0m tt_matrix \u001b[39m=\u001b[39m tt_matrix\u001b[39m.\u001b[39mtoarray()\n\u001b[1;32m---> 24\u001b[0m vocab \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39;49mget_feature_names()\n\u001b[0;32m     25\u001b[0m tt_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(np\u001b[39m.\u001b[39mround(tt_matrix, \u001b[39m2\u001b[39m), columns\u001b[39m=\u001b[39mvocab)\n\u001b[0;32m     26\u001b[0m display(tt_df)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
          ]
        }
      ],
      "source": [
        "norm_corpus = ['sky blue beautiful', 'love blue beautiful sky',\n",
        " 'quick brown fox jumps lazy dog',\n",
        " 'kings breakfast sausages ham bacon eggs toast beans',\n",
        " 'love green eggs ham sausages bacon', 'brown fox quick blue dog lazy',\n",
        " 'sky blue sky beautiful today' ,'dog lazy brown fox quick']\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# get bag of words features in sparse format\n",
        "cv = CountVectorizer(min_df=0., max_df=1.)\n",
        "cv_matrix = cv.fit_transform(norm_corpus)\n",
        "cv_matrix\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "\n",
        "\"\"\"Note: With Tfidftransformer you will systematically compute word counts using CountVectorizer\n",
        "and then compute the Inverse Document Frequency (IDF) values and only then compute the Tf-idf scores.\"\"\"\n",
        "tt = TfidfTransformer(norm='l2',\n",
        "                      use_idf=True,\n",
        "                      smooth_idf=True)\n",
        "tt_matrix = tt.fit_transform(cv_matrix)\n",
        "tt_matrix = tt_matrix.toarray()\n",
        "vocab = cv.get_feature_names()\n",
        "tt_df = pd.DataFrame(np.round(tt_matrix, 2), columns=vocab)\n",
        "display(tt_df)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Note: WWith Tfidfvectorizer on the contrary, you will do all three steps at once.\n",
        "Under the hood, it computes the word counts, IDF values, and Tf-idf scores all using the same dataset.\"\"\"\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tv = TfidfVectorizer(min_df=0.,\n",
        "                     max_df=1.,\n",
        "                     norm='l2',\n",
        "                     use_idf=True,\n",
        "                     smooth_idf=True)\n",
        "tv_matrix = tv.fit_transform(norm_corpus)\n",
        "tv_matrix = tv_matrix.toarray()\n",
        "\n",
        "vocab = tv.get_feature_names()\n",
        "tv_df  = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n",
        "display(tv_df)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xfXbJjDdV5V4"
      },
      "source": [
        "### 1.3.4 Document Similarity and Word Semantics\n",
        "\n",
        "Lexical semantics is a branch of linguistics focused on meaning and word relationships. Moreover, the idea behind word sense is the interpretation of the word (often requiring context to understand). Where multiple meanings can occur for a word â€“ take the example of a mouse that can mean both the cursor controller and the rodentâ€”we must discern using context. Relationships between word senses can be referred to as synonyms (e.g., couch/sofa).\n",
        "\n",
        "**Word Similarity** is not the same as a synonym, rather it is the idea that words have relationships. The example of cat and dog is used to show that while they are not synonymous, they are both animals, often they are domesticated â€“ their semantics are similar.\n",
        "\n",
        "**Word relatedness** is slightly different than word similarity where there is more of a psychological associationâ€”for example, that coffee and cup are related.\n",
        "Recall that vectors for representing words are called embeddings implying that a point in space can be mapped to another point in space.\n",
        "\n",
        "This is important because word similarity (measured through a vector representing distance between two words in space) can be powerful for tasks we have previously done, such as sentiment analysis. Moreover, we can derive the meaning of the word using the nearby counts of similar words.\n",
        "\n",
        "We look at three similarity metrics to score word and/or document similarity:\n",
        "\n",
        "*   Manhattan Distance: is the sum of absolute differences between points across all the dimensions. Called \"Manhattan\" because we can think of getting from point (a,b) to point (c,d) on a Cartesian plane by only travelining vertically or horizontally, not diagnally.\n",
        "*   Euclidian Distance: is the shortest distance between two points in mathmatics. Not as useful in the field of NLP. The \"as the crow flies\" distance.\n",
        "*   Cosine Similarity: measure similarity based on the content overlap between documents.\n",
        "*   Jaccard Similarity: Used to identify documents we measure it as proportion of number of common words to number of unique words in both documents.\n",
        "\n",
        "Note: Generally speaking the difference betweem *distance* and *similarity* is basically that distance is just equal to 1 - similarity.\n",
        "\n",
        "\n",
        "Let's take a look at Cosine Similarity metrics since this is most commonly used with NLP and also with word2vec.\n",
        "\n",
        "> $similarity(doc_1, doc_2) = cos(\\theta) = \\frac{doc_1  doc_2}{\\lvert doc_1\\rvert \\lvert doc_2\\rvert}$\n",
        "\n",
        "By cosine distance/dissimilarity we assume following:\n",
        "> $distance(doc_1, doc_2) = 1 - similarity(doc_1, doc_2)$\n",
        "\n",
        "The similarity-based metics look like the followingðŸ‡°\n",
        "> cos(\\vec{x},\\vec{y}) = \\frac{\\sum_{i=1}^{n}{x_i\\times y_i}}{\\sqrt{\\sum_{i=1}^{n}x_i^2}\\times \\sqrt{\\sum_{i=1}^{n}y_i^2}}\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "cosine_similarity(xyz)\n",
        "array([[1.        , 0.97780241, 0.30320366],\n",
        "       [0.97780241, 1.        , 0.49613894],\n",
        "       [0.30320366, 0.49613894, 1.        ]])\n",
        "```\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1c9D33toCdC1W3_SRiUawykcspho8IVfI)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aBFAAHro4KyS"
      },
      "source": [
        "### **1.3.5: Exercise: BOW with n-gram**\n",
        "Use the *brown* corpus to create a n-gram BOW model. First, you must clean and organize the data. Then enter your code to complete the exercise.\n",
        "\n",
        "The Brown Corpus is an collection of text samples of American English categorized by various genres such as science-fiction, adventure, etc.\n",
        "\n",
        "Create a tri-gram bag of words matrix using the brown corpus as its inputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrrT-i-u4_7M"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\shengjie\n",
            "[nltk_data]     zhao\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package brown to C:\\Users\\shengjie\n",
            "[nltk_data]     zhao\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abandon completely mythological</th>\n",
              "      <th>abandon part since</th>\n",
              "      <th>abandon sense story</th>\n",
              "      <th>abandon ship given</th>\n",
              "      <th>abandoned career bar</th>\n",
              "      <th>abandoned felt hoped</th>\n",
              "      <th>abandoned property act</th>\n",
              "      <th>abandoned role engage</th>\n",
              "      <th>abandoned shortly entire</th>\n",
              "      <th>abandoning public education</th>\n",
              "      <th>...</th>\n",
              "      <th>zing slippery know</th>\n",
              "      <th>zion said uncomfortably</th>\n",
              "      <th>zion stayed get</th>\n",
              "      <th>zodiacal light gegenschein</th>\n",
              "      <th>zoned classification similar</th>\n",
              "      <th>zones around naval</th>\n",
              "      <th>zoo averaged inches</th>\n",
              "      <th>zoo live quetzal</th>\n",
              "      <th>zoo wife rare</th>\n",
              "      <th>zq equal one</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 66482 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   abandon completely mythological  abandon part since  abandon sense story  \\\n",
              "0                                0                   0                    0   \n",
              "1                                0                   0                    0   \n",
              "2                                0                   0                    0   \n",
              "3                                0                   0                    0   \n",
              "4                                0                   0                    0   \n",
              "\n",
              "   abandon ship given  abandoned career bar  abandoned felt hoped  \\\n",
              "0                   0                     0                     0   \n",
              "1                   0                     0                     0   \n",
              "2                   0                     0                     0   \n",
              "3                   0                     0                     0   \n",
              "4                   0                     0                     0   \n",
              "\n",
              "   abandoned property act  abandoned role engage  abandoned shortly entire  \\\n",
              "0                       0                      0                         0   \n",
              "1                       0                      0                         0   \n",
              "2                       0                      0                         0   \n",
              "3                       0                      0                         0   \n",
              "4                       0                      0                         0   \n",
              "\n",
              "   abandoning public education  ...  zing slippery know  \\\n",
              "0                            0  ...                   0   \n",
              "1                            0  ...                   0   \n",
              "2                            0  ...                   0   \n",
              "3                            0  ...                   0   \n",
              "4                            0  ...                   0   \n",
              "\n",
              "   zion said uncomfortably  zion stayed get  zodiacal light gegenschein  \\\n",
              "0                        0                0                           0   \n",
              "1                        0                0                           0   \n",
              "2                        0                0                           0   \n",
              "3                        0                0                           0   \n",
              "4                        0                0                           0   \n",
              "\n",
              "   zoned classification similar  zones around naval  zoo averaged inches  \\\n",
              "0                             0                   0                    0   \n",
              "1                             0                   0                    0   \n",
              "2                             0                   0                    0   \n",
              "3                             0                   0                    0   \n",
              "4                             0                   0                    0   \n",
              "\n",
              "   zoo live quetzal  zoo wife rare  zq equal one  \n",
              "0                 0              0             0  \n",
              "1                 0              0             0  \n",
              "2                 0              0             0  \n",
              "3                 0              0             0  \n",
              "4                 0              0             0  \n",
              "\n",
              "[5 rows x 66482 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd                        # Python library for pandas - data maniplation\n",
        "import numpy as np                         # Python library for numpy -- matrix algebra library\n",
        "import matplotlib                          # Python library for matplotlib -- visual display of data\n",
        "import matplotlib.pyplot as plt            # Python library for matplotlib -- visual display of data\n",
        "import nltk                                # Python library for NLP\n",
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "\n",
        "nltk.download('stopwords')                 # package for stop words\n",
        "nltk.download('brown')                 # package for stop words\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "from nltk.corpus import brown              # this is the corpus you use for this exercise.\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#The seed() method is used to initialize the random number generator\n",
        "np.random.seed(100)\n",
        "\n",
        "brown_cat= brown.categories() # Creates a list of categories\n",
        "\n",
        "docs=[]\n",
        "for cat in brown_cat: # We append tuples of each document and categories in a list\n",
        "    t1=brown.sents(categories=cat) # At each iteration we retrieve all documents of a given category\n",
        "    for doc in t1:\n",
        "        docs.append((' '.join(doc), cat)) # These documents are appended as a tuple (document, category) in the list\n",
        "\n",
        "brown_df=pd.DataFrame(docs, columns=['sentence', 'category']) #The data frame is created using the generated tuple.\n",
        "\n",
        "brown_df.head()\n",
        "\n",
        "\n",
        "#Step 1. Pre-Processing the Brown Corpus Text\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I | re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = wpt.tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)\n",
        "\n",
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "# Setting the random seed\n",
        "np.random.seed(100)\n",
        "\n",
        "# Step 1: Normalize the corpus\n",
        "normalize_corpus = np.vectorize(normalize_document)\n",
        "\n",
        "# Applying the normalization\n",
        "norm_corpus = normalize_corpus(list(brown_df['sentence']))\n",
        "\n",
        "# Because the brown corpus is very large, select 10,000 random records from the corpus\n",
        "num_samples = 10000\n",
        "rand_samples = np.random.choice(norm_corpus, size=num_samples)\n",
        "\n",
        "# Step 2: Create a tri-gram model and count its frequencies\n",
        "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
        "\n",
        "# Fit the vectorizer to the data\n",
        "X = vectorizer.fit_transform(rand_samples)\n",
        "\n",
        "# Get the feature names (vocab)\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the dataframe to show the tri-gram BOW\n",
        "df = pd.DataFrame(X.toarray(), columns=vocab)\n",
        "df.head()\n",
        "\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m1I-GlKzVk7B"
      },
      "source": [
        "### 1.3.6 Exercise: TD-IDF\n",
        "\n",
        "Now, using anyone of the following datasets, create you're own TF-IDF implementation. Provide your output in the form of a matrix.\n",
        "\n",
        "For more on the intuition behind TF-IDF, read the article [here](https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/text-vec-traditional.html).\n",
        "\n",
        "Refer to [this article](https://sci2lab.github.io/ml_tutorial/tfidf/) related to TF-IDF and Elastisearch. Note how the TF-IDF approach can be used for information retrieval.\n",
        "\n",
        "Datasets that you may choose from:\n",
        "*   [Reviews Dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). this dataset uses classified data from Yelp!, Amazon, and IMBD. You can use this to determine TF-IDF across the datasets.\n",
        "\n",
        "\n",
        "*   Presidential speeches in NLTK. You can use this dataset to determine the TFIDF vector of words across presidential speeches.\n",
        "\n",
        "```\n",
        "nltk.corpus.inaugural\n",
        "```\n",
        "\n",
        "Please provide your code in the cell below.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bckx5RzyUob"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package inaugural to C:\\Users\\shengjie\n",
            "[nltk_data]     zhao\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           1789-Washington.txt  1793-Washington.txt  1797-Adams.txt  \\\n",
            "000                        0.0                  0.0        0.000000   \n",
            "100                        0.0                  0.0        0.000000   \n",
            "108                        0.0                  0.0        0.000000   \n",
            "11                         0.0                  0.0        0.000000   \n",
            "120                        0.0                  0.0        0.000000   \n",
            "...                        ...                  ...             ...   \n",
            "youÃ¢                       0.0                  0.0        0.000000   \n",
            "zeal                       0.0                  0.0        0.009473   \n",
            "zealous                    0.0                  0.0        0.000000   \n",
            "zealously                  0.0                  0.0        0.000000   \n",
            "zone                       0.0                  0.0        0.000000   \n",
            "\n",
            "           1801-Jefferson.txt  1805-Jefferson.txt  1809-Madison.txt  \\\n",
            "000                  0.000000            0.000000          0.000000   \n",
            "100                  0.000000            0.000000          0.000000   \n",
            "108                  0.000000            0.000000          0.000000   \n",
            "11                   0.000000            0.000000          0.000000   \n",
            "120                  0.000000            0.000000          0.000000   \n",
            "...                       ...                 ...               ...   \n",
            "youÃ¢                 0.000000            0.000000          0.000000   \n",
            "zeal                 0.012663            0.032769          0.000000   \n",
            "zealous              0.000000            0.000000          0.000000   \n",
            "zealously            0.000000            0.000000          0.018453   \n",
            "zone                 0.000000            0.000000          0.000000   \n",
            "\n",
            "           1813-Madison.txt  1817-Monroe.txt  1821-Monroe.txt  1825-Adams.txt  \\\n",
            "000                     0.0         0.000000         0.026986        0.000000   \n",
            "100                     0.0         0.000000         0.000000        0.000000   \n",
            "108                     0.0         0.000000         0.000000        0.000000   \n",
            "11                      0.0         0.000000         0.000000        0.000000   \n",
            "120                     0.0         0.000000         0.007879        0.000000   \n",
            "...                     ...              ...              ...             ...   \n",
            "youÃ¢                    0.0         0.000000         0.000000        0.000000   \n",
            "zeal                    0.0         0.006705         0.000000        0.006311   \n",
            "zealous                 0.0         0.007644         0.005912        0.000000   \n",
            "zealously               0.0         0.000000         0.005636        0.000000   \n",
            "zone                    0.0         0.010186         0.000000        0.000000   \n",
            "\n",
            "           ...  1985-Reagan.txt  1989-Bush.txt  1993-Clinton.txt  \\\n",
            "000        ...              0.0            0.0               0.0   \n",
            "100        ...              0.0            0.0               0.0   \n",
            "108        ...              0.0            0.0               0.0   \n",
            "11         ...              0.0            0.0               0.0   \n",
            "120        ...              0.0            0.0               0.0   \n",
            "...        ...              ...            ...               ...   \n",
            "youÃ¢       ...              0.0            0.0               0.0   \n",
            "zeal       ...              0.0            0.0               0.0   \n",
            "zealous    ...              0.0            0.0               0.0   \n",
            "zealously  ...              0.0            0.0               0.0   \n",
            "zone       ...              0.0            0.0               0.0   \n",
            "\n",
            "           1997-Clinton.txt  2001-Bush.txt  2005-Bush.txt  2009-Obama.txt  \\\n",
            "000                     0.0            0.0            0.0             0.0   \n",
            "100                     0.0            0.0            0.0             0.0   \n",
            "108                     0.0            0.0            0.0             0.0   \n",
            "11                      0.0            0.0            0.0             0.0   \n",
            "120                     0.0            0.0            0.0             0.0   \n",
            "...                     ...            ...            ...             ...   \n",
            "youÃ¢                    0.0            0.0            0.0             0.0   \n",
            "zeal                    0.0            0.0            0.0             0.0   \n",
            "zealous                 0.0            0.0            0.0             0.0   \n",
            "zealously               0.0            0.0            0.0             0.0   \n",
            "zone                    0.0            0.0            0.0             0.0   \n",
            "\n",
            "           2013-Obama.txt  2017-Trump.txt  2021-Biden.txt  \n",
            "000                   0.0             0.0        0.010530  \n",
            "100                   0.0             0.0        0.000000  \n",
            "108                   0.0             0.0        0.015372  \n",
            "11                    0.0             0.0        0.015372  \n",
            "120                   0.0             0.0        0.000000  \n",
            "...                   ...             ...             ...  \n",
            "youÃ¢                  0.0             0.0        0.015372  \n",
            "zeal                  0.0             0.0        0.000000  \n",
            "zealous               0.0             0.0        0.000000  \n",
            "zealously             0.0             0.0        0.000000  \n",
            "zone                  0.0             0.0        0.000000  \n",
            "\n",
            "[9261 rows x 59 columns]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data]   Unzipping corpora\\inaugural.zip.\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import inaugural\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('inaugural')\n",
        "\n",
        "file_ids = inaugural.fileids()\n",
        "documents = [inaugural.words(fileid) for fileid in file_ids]\n",
        "documents = [\" \".join(doc).lower() for doc in documents]\n",
        "\n",
        "# TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "#dataframe\n",
        "df = pd.DataFrame(tfidf_matrix.T.todense(), index=feature_names, columns=file_ids)\n",
        "\n",
        "print(df)\n",
        "\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QEGeTp9idVUk"
      },
      "source": [
        "##A. References\n",
        "\n",
        "1.   Chapter 6 â€“ Vector Semantics and Word Embeddings Speech and Language rocessing. Daniel Jurafsky & James H. Martin. Copyright Â© 2021. All rights reserved. Draft of September 21, 2021.\n",
        "2.   [Word2vec from Scratch with NumPy](https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72)\n",
        "3.   [A hands=on intutive approach to Deep Learning Methods for Text Data - Word2Vec,GloVe and FastText](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)\n",
        "4.    [Traditional Methods for Text Data](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)\n",
        "5.    [Word Embeddings](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/guide/word_embeddings.ipynb#scrollTo=Q6mJg1g3apaz)\n",
        "6. [CS 224D: Deep Learning for NLP](https://cs224d.stanford.edu/lecture_notes/LectureNotes1.pdf)\n",
        "7. [Text Vectorization] (https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/text-vec-traditional.html)\n",
        "8. [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus)\n",
        "9. [TF-IDF](https://ethen8181.github.io/machine-learning/clustering_old/tf_idf/tf_idf.html)\n",
        "10. [Applying TF-IDF algorithm in practice](https://plumbr.io/blog/programming/applying-tf-idf-algorithm-in-practice)\n",
        "11. [text2vec](http://text2vec.org/similarity.html)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOyZQBiOWk75J27YoRK1g7z",
      "include_colab_link": true,
      "mount_file_id": "14hmD2J17784H_zxVA2HN9HEEuR6vt65C",
      "name": "M2_Assignment_Part_I.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
